{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd37925-f2b8-4383-9d92-3183cfd7a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import scipy\n",
    "# from scipy import interpolate, sparse\n",
    "\n",
    "import icesat2_toolkit.fit\n",
    "import icesat2_toolkit.time\n",
    "import icesat2_toolkit.utilities\n",
    "\n",
    "from classify_photons import classify_photons\n",
    "# from yapc.classify_photons import classify_photons\n",
    "\n",
    "\n",
    "import h5py\n",
    "import warnings\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035e03b-ef6f-4c32-8da0-f79c3a38fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = pd.DataFrame()\n",
    "\n",
    "# # print(fi['gt1l']['heights'].keys())\n",
    "# # <KeysViewHDF5 ['delta_time', 'dist_ph_across', 'dist_ph_along', 'h_ph', 'lat_ph', 'lon_ph', 'pce_mframe_cnt', 'ph_id_channel', 'ph_id_count', 'ph_id_pulse', 'quality_ph', 'signal_conf_ph']>\n",
    "\n",
    "# with h5py.File(fileID, 'r') as fi:\n",
    "#     for key in fi['gt1l']['heights'].keys():\n",
    "#         dset.insert('key', key[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d18a8f-f992-405c-b660-b48f237e3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileID06 = \"228878251/ATL06_20200212232313_07370602_005_01.h5\"\n",
    "mds06 = {}\n",
    "with h5py.File(fileID06, 'r') as fi:\n",
    "\n",
    "    print(fi['gt1l'].keys())\n",
    "    for top_key in fi[beam].keys():\n",
    "        if (top_key != 'signal_find_output'): ### if instance of group?\n",
    "            mds[top_key] = {}\n",
    "\n",
    "                #ph_lat, ph_lon, etc.\n",
    "            for key in fi[beam][top_key].keys():\n",
    "                dataset = fi[beam][top_key].get(key) #(val) dataset will get closed w file\n",
    "                mds[top_key][key] = dataset[:] #so copying dataset into memory\n",
    "    \n",
    "    # for key in fi['gt1l']['heights'].keys():\n",
    "    #     dset.insert('key', key[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10f8f6-2298-4eae-b916-b17596d8abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readATL03(fileID, beam, bbox):\n",
    "    \n",
    "    mds = {}\n",
    "    \n",
    "    with h5py.File(fileID, 'r') as fi:\n",
    "\n",
    "        for top_key in fi[beam].keys():\n",
    "            if (top_key != 'signal_find_output'): ### if instance of group?\n",
    "                mds[top_key] = {}\n",
    "\n",
    "                #ph_lat, ph_lon, etc.\n",
    "                for key in fi[beam][top_key].keys():\n",
    "                    dataset = fi[beam][top_key].get(key) #(val) dataset will get closed w file\n",
    "                    mds[top_key][key] = dataset[:] #so copying dataset into memory\n",
    "                    \n",
    "        # print(min(mds['heights']['lon_ph']),max(mds['heights']['lon_ph']))  \n",
    "        # print(min(mds['heights']['lat_ph']),max(mds['heights']['lat_ph']))        \n",
    "\n",
    "        ##SUBSETTING HEIGHT AND GEOLOCATION:\n",
    "        \n",
    "        #if a lat/lon bounding box:\n",
    "        if (len(bbox) == 4):\n",
    "            w,e,s,n = bbox\n",
    "        \n",
    "            #boolean mask of all heights values (across keys) that satisfy bbox restrictions\n",
    "            mask = (mds['heights']['lon_ph'] >= w) & (mds['heights']['lon_ph'] <= e) & \\\n",
    "                    (mds['heights']['lat_ph'] >= s) & (mds['heights']['lat_ph'] <= n)\n",
    "        \n",
    "        #else if a dt bounding box:        \n",
    "        elif (len(bbox) == 2):\n",
    "            dt_min, dt_max = bbox\n",
    "            \n",
    "            mask = (mds['heights']['delta_time'] >= dt_min) & (mds['heights']['delta_time'] <= dt_max) \n",
    "        \n",
    "        #for pond 1 e.g. goes from 10730648 'heights' values to 23033 with the mask :)\n",
    "        for k, v in mds['heights'].items():\n",
    "            mds['heights'][k] = v[mask]    \n",
    "            \n",
    "        \n",
    "        dt_min, dt_max = min(mds['heights']['delta_time']), max(mds['heights']['delta_time'])\n",
    "\n",
    "        geo_mask = (mds['geolocation']['delta_time'] >= dt_min) & (mds['geolocation']['delta_time'] <= dt_max) \n",
    "        for k, v in mds['geolocation'].items():\n",
    "            mds['geolocation'][k] = v[geo_mask]\n",
    "        \n",
    "        atlas_mask = (mds['bckgrd_atlas']['delta_time'] >= dt_min) & (mds['bckgrd_atlas']['delta_time'] <= dt_max) \n",
    "        for k, v in mds['bckgrd_atlas'].items():\n",
    "            mds['bckgrd_atlas'][k] = v[atlas_mask]\n",
    "        \n",
    "        mds['orbit_info'] = {}\n",
    "        for key,val in fi['orbit_info'].items(): \n",
    "            mds['orbit_info'][key] = val[:]\n",
    "            \n",
    "        mds['ancillary_data'] = {}\n",
    "        ancillary_keys = ['atlas_sdp_gps_epoch','data_end_utc','data_start_utc',\n",
    "            'end_cycle','end_geoseg','end_gpssow','end_gpsweek','end_orbit',\n",
    "            'end_region','end_rgt','granule_end_utc','granule_start_utc','release',\n",
    "            'start_cycle','start_geoseg','start_gpssow','start_gpsweek',\n",
    "            'start_orbit','start_region','start_rgt','version']\n",
    "        for key in ancillary_keys:\n",
    "            #-- get each HDF5 variable\n",
    "            mds['ancillary_data'][key] = fi['ancillary_data'][key][:]\n",
    "                \n",
    "         #-- transmit-echo-path (tep) parameters\n",
    "        mds['ancillary_data']['tep'] = {}\n",
    "        for key,val in fi['ancillary_data']['tep'].items():\n",
    "            #-- get each HDF5 variable\n",
    "            mds['ancillary_data']['tep'][key] = val[:]\n",
    "       \n",
    "        #-- channel dead time and first photon bias derived from ATLAS calibration\n",
    "        cal1,cal2 = ('ancillary_data','calibrations')\n",
    "        for var in ['dead_time','first_photon_bias']:\n",
    "            mds[cal1][var] = {}\n",
    "            for key,val in fi[cal1][cal2][var].items():\n",
    "                #-- get each HDF5 variable\n",
    "                if isinstance(val, h5py.Dataset):\n",
    "                    mds[cal1][var][key] = val[:]\n",
    "                elif isinstance(val, h5py.Group):\n",
    "                    mds[cal1][var][key] = {}\n",
    "                    for k,v in val.items():\n",
    "                        mds[cal1][var][key][k] = v[:]\n",
    "\n",
    "                #-- get ATLAS impulse response variables for the transmitter echo path (TEP)\n",
    "                tep1,tep2 = ('atlas_impulse_response','tep_histogram')\n",
    "                mds[tep1] = {}\n",
    "                for pce in ['pce1_spot1','pce2_spot3']:\n",
    "                    mds[tep1][pce] = {tep2:{}}\n",
    "                    #-- for each TEP variable\n",
    "                    for key,val in fi[tep1][pce][tep2].items():\n",
    "                        mds[tep1][pce][tep2][key] = val[:]\n",
    "        \n",
    "        #-- Return the datasets and variables before HDF5 file closes automatically (not sure if indent needed)\n",
    "        return mds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555a4dd-84dd-48a6-814e-c8eaa051dc66",
   "metadata": {},
   "source": [
    "# Read ATL03 file for a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37641b31-1ec9-449b-a72c-311f798e9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,s,e,n= -108.42635020791396, 38.788639821001155, -107.68485163209928, 39.34603060272382\n",
    "bbox = w,e,s,n\n",
    "beam = 'gt1r'\n",
    "\n",
    "fileID = \"225517609/ATL03_20200212232313_07370602_005_01.h5\"\n",
    "gt1_mds = readATL03(fileID, beam, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3b9fa-bd18-429c-8f6b-7b8e0ad172e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt1_mds['heights']['lon_ph'], gt1_mds['heights']['lat_ph'])#,color=gt1_mds['heights']['h_ph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bb5f9-59c9-4b41-a3c7-0f4aa8c5c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt1_mds['heights'].keys()\n",
    "np.unique(gt1_mds['heights']['quality_ph'])#[0, 1, 2]\n",
    "np.unique(gt1_mds['heights']['signal_conf_ph'])#[-1, 0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee4838-ce22-4200-b463-3f8c845f072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_avg = np.average(gt1_mds['heights']['h_ph'][:])\n",
    "bds = [min(gt1_mds['heights']['lat_ph']), max(gt1_mds['heights']['lat_ph'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe284f-063c-4b0c-956f-028a2753b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMDS(gt1_mds, bds, h_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf9d7b-06b5-4f5e-9f31-8ec5183e7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMDS(gt1_mds, [39.05,39.1], 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e5339-8e51-4b36-b0d8-af7de04d2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tsutterley/read-ICESat-2/blob/main/notebooks/Read%20ICESat-2%20ATL03.ipynb\n",
    "#@params: data (mds) and attributes\n",
    "def seg_data(mds):\n",
    "    \n",
    "    #-- number of GPS seconds between the GPS epoch\n",
    "    #-- and ATLAS Standard Data Product (SDP) epoch\n",
    "    print(mds['ancillary_data']['atlas_sdp_gps_epoch'][0:1])\n",
    "    atlas_sdp_gps_epoch = mds['ancillary_data']['atlas_sdp_gps_epoch'][:]\n",
    "    \n",
    "    #-- variables of interest for generating corrected elevation estimates:\n",
    "    #-- ATL03 Segment ID\n",
    "    Segment_ID = mds['geolocation']['segment_id']  \n",
    "    n_seg = len(Segment_ID) #183\n",
    "    #-- number of photon events\n",
    "    n_pe, = mds['heights']['delta_time'].shape #Pond 1 -> 23033\n",
    "    #-- first photon in the segment (convert to 0-based indexing)\n",
    "    Segment_Index_begin = mds['geolocation']['ph_index_beg'] - 1\n",
    "    #-- number of photon events in the segment:\n",
    "    Segment_PE_count = mds['geolocation']['segment_ph_cnt']    \n",
    "\n",
    "    #-- along-track distance for each ATL03 segment\n",
    "    Segment_Distance = mds['geolocation']['segment_dist_x']    \n",
    "    #-- along-track length for each ATL03 segment\n",
    "    Segment_Length = mds['geolocation']['segment_length']\n",
    "\n",
    "    #-- background photon rate\n",
    "    #-- Transmit time of the reference photon\n",
    "    delta_time = mds['geolocation']['delta_time']\n",
    "    #-- interpolate background photon rate based on 50-shot summation\n",
    "    background_delta_time = mds['bckgrd_atlas']['delta_time']\n",
    "    SPL = scipy.interpolate.UnivariateSpline(background_delta_time,\n",
    "                                             mds['bckgrd_atlas']['bckgrd_rate'],k=3,s=0)\n",
    "    background_rate = SPL(delta_time)\n",
    "    \n",
    "    #-- along-track and across-track distance for photon events\n",
    "    #atc = along/across track coordinates\n",
    "    x_atc = mds['heights']['dist_ph_along'] # meters [20.090918 19.74102  20.044334 ... 16.128172 16.12856  16.129326]\n",
    "    y_atc = mds['heights']['dist_ph_across']\n",
    "    #-- photon event heights\n",
    "    h_ph = mds['heights']['h_ph'][:]\n",
    "    \n",
    "    #-- for each 20m segment:\n",
    "    ph_start_idx = Segment_Index_begin[0]\n",
    "    for j,_ in enumerate(Segment_ID):\n",
    "        #-- index for 20m segment j\n",
    "        idx = Segment_Index_begin[j] - ph_start_idx #e.g. 447696\n",
    "        #-- skip segments with no photon events\n",
    "        if (idx < 0):\n",
    "            continue\n",
    "        #-- number of photons in 20m segment\n",
    "        cnt = Segment_PE_count[j] #e.g. 480\n",
    "        #-- add segment distance to along-track coordinates\n",
    "        x_atc[idx:idx+cnt] += Segment_Distance[j] #fixed 6.30\n",
    "\n",
    "    #-- iterate over ATLAS major frames\n",
    "    photon_mframes = mds['heights']['pce_mframe_cnt']\n",
    "    pce_mframe_cnt = mds['bckgrd_atlas']['pce_mframe_cnt']\n",
    "    unique_major_frames,unique_index = np.unique(pce_mframe_cnt,return_index=True)\n",
    "    major_frame_count = len(unique_major_frames)\n",
    "    tlm_height_band1 = mds['bckgrd_atlas']['tlm_height_band1']\n",
    "    tlm_height_band2 = mds['bckgrd_atlas']['tlm_height_band2']\n",
    "    \n",
    "    #-- photon event signal-to-noise ratio from photon classifier\n",
    "    pe_weights = np.zeros((n_pe),dtype=np.float64)\n",
    "    Photon_SNR = np.zeros((n_pe),dtype=int)\n",
    "    #-- run for each major frame\n",
    "    for iteration,idx in enumerate(unique_index):\n",
    "        #-- sum of 2 telemetry band widths for major frame\n",
    "        h_win_width = tlm_height_band1[idx] + tlm_height_band2[idx]\n",
    "        #-- photon indices for major frame (buffered by 1 on each side)\n",
    "        i1, = np.nonzero((photon_mframes >= unique_major_frames[iteration]-1) &\n",
    "            (photon_mframes <= unique_major_frames[iteration]+1))\n",
    "        #-- indices for the major frame within the buffered window\n",
    "        i2, = np.nonzero(photon_mframes[i1] == unique_major_frames[iteration])\n",
    "        #-- calculate photon event weights\n",
    "        pe_weights[i1[i2]] = classify_photons(x_atc[i1], h_ph[i1],\n",
    "            h_win_width, i2, K=5, MIN_PH=5, MIN_XSPREAD=1.0,\n",
    "            MIN_HSPREAD=0.01, METHOD='linear')\n",
    "\n",
    "    #-- allocate for segment means\n",
    "    fill_value = np.nan\n",
    "    #-- mean longitude of each segment high-confidence photons\n",
    "    Segment_Lon = np.ma.zeros((n_seg),fill_value=fill_value)\n",
    "    Segment_Lon.data[:] = Segment_Lon.fill_value\n",
    "    Segment_Lon.mask = np.ones((n_seg),dtype=bool)\n",
    "    #-- mean longitude of each segment high-confidence photons\n",
    "    Segment_Lat = np.ma.zeros((n_seg),fill_value=fill_value)\n",
    "    Segment_Lat.data[:] = Segment_Lat.fill_value\n",
    "    Segment_Lat.mask = np.ones((n_seg),dtype=bool)\n",
    "    #-- mean height of each segment high-confidence photons\n",
    "    Segment_Elev = np.ma.zeros((n_seg),fill_value=fill_value)\n",
    "    Segment_Elev.data[:] = Segment_Elev.fill_value\n",
    "    Segment_Elev.mask = np.ones((n_seg),dtype=bool)\n",
    "    #-- mean time of each segment high-confidence photons\n",
    "    Segment_Time = np.ma.zeros((n_seg),fill_value=fill_value)\n",
    "    Segment_Time.data[:] = Segment_Time.fill_value\n",
    "    Segment_Time.mask = np.ones((n_seg),dtype=bool)\n",
    "    \n",
    "    #segment SNR/PE confidence (added 7.12)\n",
    "    Segment_Photon_SNR = np.ma.zeros((n_seg), fill_value = fill_value)\n",
    "    Segment_Photon_SNR.data[:] = Segment_Photon_SNR.fill_value\n",
    "    Segment_Photon_SNR.mask = np.ones((n_seg), dtype=bool)\n",
    "    \n",
    "    #-- iterate over ATL03 segments to calculate 40m means\n",
    "    #-- in ATL03 1-based indexing: invalid == 0\n",
    "    #-- here in 0-based indexing: invalid == -1   \n",
    "    segment_indices, = np.nonzero((Segment_Index_begin[:-1] >= 0) &\n",
    "        (Segment_Index_begin[1:] >= 0))\n",
    "    ###FIX 6.30; index references photon event numbers (first in our mds = 447696)\n",
    "    ###but that's 0, for indexing purposes\n",
    "    ph_start_idx = Segment_Index_begin[0]\n",
    "    #print(\"The following segments were masked due to not enough photons events \\\n",
    "    #        above low-confidence threshold (10) and/or spread of photons in segment < 20m:\")\n",
    "    for j in segment_indices: \n",
    "        #-- index for segment j\n",
    "        idx = Segment_Index_begin[j] - ph_start_idx #6.30 ***** CHANGING TO EQUAL J, SEE 20m SECTION ABOVE\n",
    "        #-- number of photons in segment (use 2 ATL03 segments)\n",
    "        c1 = np.copy(Segment_PE_count[j])\n",
    "        c2 = np.copy(Segment_PE_count[j+1])\n",
    "        cnt = c1 + c2\n",
    "        #-- time of each Photon event (PE)\n",
    "        segment_delta_times = mds['heights']['delta_time'][idx:idx+cnt]\n",
    "        gps_seconds = atlas_sdp_gps_epoch + segment_delta_times\n",
    "        time_leaps = icesat2_toolkit.time.count_leap_seconds(gps_seconds)\n",
    "        #-- Photon event lat/lon and elevation (WGS84)\n",
    "        segment_heights = h_ph[idx:idx+cnt].copy()\n",
    "        segment_lats = mds['heights']['lat_ph'][idx:idx+cnt]\n",
    "        segment_lons = mds['heights']['lon_ph'][idx:idx+cnt]\n",
    "        #-- calculate segment time in Julian days (UTC)\n",
    "        segment_times = 2444244.5 + (gps_seconds - time_leaps)/86400.0           \n",
    "        #-- Photon event channel and identification\n",
    "        ID_channel = mds['heights']['ph_id_channel'][idx:idx+cnt]\n",
    "        ID_pulse = mds['heights']['ph_id_pulse'][idx:idx+cnt]\n",
    "        n_pulses = np.unique(ID_pulse).__len__()\n",
    "        frame_number = mds['heights']['pce_mframe_cnt'][idx:idx+cnt]\n",
    "        #-- along-track X and Y coordinates\n",
    "        distance_along_X = np.copy(x_atc[idx:idx+cnt])\n",
    "        distance_along_Y = np.copy(y_atc[idx:idx+cnt])\n",
    "        #-- check the spread of photons along-track (must be > 20m)\n",
    "        along_X_spread = distance_along_X.max() - distance_along_X.min()\n",
    "        #-- Along-track distance between 2 segments\n",
    "        X_atc = Segment_Distance[j] + Segment_Length[j]        \n",
    "        #-- check confidence level associated with each photon event for sea ice (2)\n",
    "        ice_sig_conf = mds['heights']['signal_conf_ph'][idx:idx+cnt,2]\n",
    "        ice_sig_low_count = np.count_nonzero(ice_sig_conf >= 1)\n",
    "        #-- indices of TEP classified photons\n",
    "        ice_sig_tep_pe, = np.nonzero(ice_sig_conf == -2)\n",
    "        #-- photon event weights from photon classifier\n",
    "        segment_weights = pe_weights[idx:idx+cnt]\n",
    "        snr_norm = np.max(segment_weights)\n",
    "        #-- photon event signal-to-noise ratio from photon classifier\n",
    "        photon_snr = np.zeros((len(segment_weights)),dtype=int)\n",
    "        \n",
    "        if (snr_norm > 0):\n",
    "            photon_snr[:] = 100.0*segment_weights/snr_norm\n",
    "        #-- copy signal to noise ratio for photons\n",
    "        Photon_SNR[idx:idx+cnt] = np.copy(photon_snr)\n",
    "        #-- photon confidence levels from classifier\n",
    "        pe_sig_conf = np.zeros((len(segment_weights)),dtype=int)\n",
    "        #-- calculate confidence levels from photon classifier\n",
    "        pe_sig_conf[photon_snr >= 25] = 2\n",
    "        pe_sig_conf[photon_snr >= 60] = 3\n",
    "        pe_sig_conf[photon_snr >= 80] = 4\n",
    "        #-- copy classification for TEP photons\n",
    "        pe_sig_conf[ice_sig_tep_pe] = -2\n",
    "        pe_sig_low_count = np.count_nonzero(pe_sig_conf > 1)\n",
    "        #-- check if segment has photon events classified \n",
    "        #-- for sea ice that are at least low-confidence threshold\n",
    "        #-- and that the spread of photons is greater than 20m\n",
    "        if (pe_sig_low_count > 10) & (along_X_spread > 20):\n",
    "            #-- find photon events that are high-confidence\n",
    "            ii, = np.nonzero(pe_sig_conf >= 4)\n",
    "            #-- calculate mean elevation (without iterations)\n",
    "            #-- NOTE that the segment elevations will NOT be corrected\n",
    "            #-- for transmit pulse shape biases or first photon biases\n",
    "            Segment_Elev.data[j] = icesat2_toolkit.fit.fit_geolocation(\n",
    "                segment_heights[ii], distance_along_X[ii], X_atc)\n",
    "            #-- calculate geolocation and time of 40m segment center\n",
    "            Segment_Lon.data[j] = icesat2_toolkit.fit.fit_geolocation(\n",
    "                segment_lons[ii], distance_along_X[ii], X_atc)\n",
    "            Segment_Lat.data[j] = icesat2_toolkit.fit.fit_geolocation(\n",
    "                segment_lats[ii], distance_along_X[ii], X_atc)\n",
    "            Segment_Time.data[j] = icesat2_toolkit.fit.fit_geolocation(\n",
    "                segment_times[ii], distance_along_X[ii], X_atc)\n",
    "            \n",
    "        #added:7/12 (average )\n",
    "        Segment_Photon_SNR.data[j] = snr_norm#np.mean(segment_weights)\n",
    "        Segment_Photon_SNR.mask[j] = False\n",
    "            \n",
    "    #-- clear photon classifier variables for beam group\n",
    "    pe_weights = None\n",
    "    \n",
    "    #META_DICT\n",
    "    seg_data = {}\n",
    "    \n",
    "    #-- variables of interest for generating corrected elevation estimates\n",
    "    seg_data['Segment_ID'] = np.copy(Segment_ID)\n",
    "    seg_data['Segment_Index_begin'] = np.copy(Segment_Index_begin)\n",
    "    seg_data['Segment_PE_count'] = np.copy(Segment_PE_count)\n",
    "    seg_data['Segment_Distance'] = np.copy(Segment_Distance)\n",
    "    seg_data['Segment_Length'] = np.copy(Segment_Length)\n",
    "    #-- mean geolocation, height and delta time\n",
    "    seg_data['Segment_Lon'] = np.copy(Segment_Lon)\n",
    "    seg_data['Segment_Lat'] = np.copy(Segment_Lat)\n",
    "    seg_data['Segment_Elev'] = np.copy(Segment_Elev)\n",
    "    seg_data['Segment_Time'] = np.copy(Segment_Time)\n",
    "    #-- background photon rate\n",
    "    seg_data['background_rate'] = np.copy(background_rate)\n",
    "    #-- segment SNR value\n",
    "    seg_data['Segment_Photon_SNR'] = np.copy(Segment_Photon_SNR)\n",
    "    \n",
    "    #-- photon event signal-to-noise ratio from photon classifier\n",
    "    ph_SNR = np.copy(Photon_SNR)\n",
    "    \n",
    "    return seg_data, ph_SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ff984-5bb3-4242-8986-179170b00217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMDS(mds,bds, h_avg):\n",
    "            \n",
    "    ht = mds['heights']['h_ph'][:]\n",
    "    # dt = mds['heights']['delta_time'][:]\n",
    "    lat = mds['heights']['lat_ph'][:]\n",
    "    \n",
    "    #-- Signal classification confidence for sea ice\n",
    "    #-- 0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater\n",
    "    sig_conf = mds['heights']['signal_conf_ph'][:,0]\n",
    "    #**** do I need to look at any of the other classification confidences?\n",
    "    \n",
    "        #confidence level associated with each photon event: \n",
    "        #-- -2: TEP (Transmit Echo Pulse) returns\n",
    "        #-- -1: Events not associated with a specific surface type\n",
    "        #--  0: noise\n",
    "        #--  1: buffer but algorithm classifies as background\n",
    "        #--  2: low\n",
    "        #--  3: medium\n",
    "        #--  4: high\n",
    "    \n",
    "    isTEP, = np.nonzero(sig_conf == -2)\n",
    "    noSType, = np.nonzero(sig_conf == -1)\n",
    "    bkgrd, = np.nonzero((sig_conf == 0) | (sig_conf == 1)) ##includes buffer\n",
    "    #-- find photon events of progressively higher confidence\n",
    "    lc, = np.nonzero(sig_conf == 2)\n",
    "    mc, = np.nonzero(sig_conf == 3)\n",
    "    hc, = np.nonzero(sig_conf == 4)\n",
    "    \n",
    "    #PLOT:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(12,4))\n",
    "    plt.xlabel('Latitude (s)',fontsize=20)\n",
    "    plt.ylabel('Photon Heights (m)',fontsize=20)\n",
    "    ax.set_xlim(bds[0], bds[1])\n",
    "    # ax.set_ylim(h_avg - 500, h_avg+500)#(-5,10)\n",
    "    \n",
    "    size = 2.5\n",
    "    ax.plot(lat[isTEP], mds['heights']['h_ph'][isTEP],marker='.', markersize=size,lw=0,color='red',label='TEP')\n",
    "    ax.plot(lat[noSType], mds['heights']['h_ph'][noSType],marker='.', markersize=size,lw=0,color='0.2',label='No Surface Classification')\n",
    "    ax.plot(lat[bkgrd], mds['heights']['h_ph'][bkgrd],marker='.', markersize=size,lw=0,color='0.5',label='Background')\n",
    "    ax.plot(lat[lc], mds['heights']['h_ph'][lc],marker='.', markersize=size,lw=0,color='darkorange',label='Low Confidence')\n",
    "    ax.plot(lat[mc], mds['heights']['h_ph'][mc],marker='.', markersize=size,lw=0,color='mediumseagreen',label='Medium Confidence')\n",
    "    ax.plot(lat[hc], mds['heights']['h_ph'][hc],marker='.', markersize=size,lw=0,color='darkorchid',label='High Confidence')\n",
    "    \n",
    "    #legend\n",
    "    lgd = ax.legend(loc=3,frameon=False)\n",
    "    lgd.get_frame().set_alpha(1.0)\n",
    "    for line in lgd.get_lines():\n",
    "        line.set_linewidth(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6013fdb-ace6-491d-8cf3-98dfe882ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSNR(mds, SNR, bounds, pondID, h_avg):\n",
    "    \n",
    "    ht = mds['heights']['h_ph'][:]\n",
    "    dt = mds['heights']['delta_time'][:]\n",
    "    dt_min, dt_max = bounds\n",
    "\n",
    "    #CLASSIFICATION CONFIDENCE:\n",
    "    #-- Signal classification confidence for sea ice\n",
    "    #-- 0=Land; 1=Ocean; 2=SeaIce; 3=LandIce; 4=InlandWater\n",
    "    ice_sig_conf = mds['heights']['signal_conf_ph'][:,2]    \n",
    "    #confidence level associated with each photon event: \n",
    "    isTEP, = np.nonzero(ice_sig_conf == -2)  # -2: TEP (Transmit Echo Pulse) returns\n",
    "    noSType, = np.nonzero(ice_sig_conf == -1) #-- -1: Events not associated with a specific surface type\n",
    "    bkgrd, = np.nonzero((ice_sig_conf == 0) | (ice_sig_conf == 1)) ##0: noise, 1: buffer (aka bckgrd)\n",
    "    #-- find photon events of progressively higher confidence\n",
    "    lc, = np.nonzero(ice_sig_conf == 2) #low\n",
    "    mc, = np.nonzero(ice_sig_conf == 3) #medium\n",
    "    hc, = np.nonzero(ice_sig_conf == 4) #high\n",
    "    \n",
    "    #PLOTS:\n",
    "    fig,ax = plt.subplots(num=2,nrows=2,sharex=True,figsize=(12,8))\n",
    "    size = 7.5 #dot size\n",
    "    \n",
    "    #CONFIDENCES:   \n",
    "    ax[0].plot(dt[isTEP], mds['heights']['h_ph'][isTEP],marker='.', markersize=size,lw=0,color='red',label='TEP')\n",
    "    ax[0].plot(dt[noSType], mds['heights']['h_ph'][noSType],marker='.', markersize=size,lw=0,color='0.2',label='No Surface Classification')\n",
    "    ax[0].plot(dt[bkgrd], mds['heights']['h_ph'][bkgrd],marker='.', markersize=size,lw=0,color='0.5',label='Background')\n",
    "    ax[0].plot(dt[lc], mds['heights']['h_ph'][lc],marker='.', markersize=size,lw=0,color='darkorange',label='Low Confidence')\n",
    "    ax[0].plot(dt[mc], mds['heights']['h_ph'][mc],marker='.', markersize=size,lw=0,color='mediumseagreen',label='Medium Confidence')\n",
    "    ax[0].plot(dt[hc], mds['heights']['h_ph'][hc],marker='.', markersize=size,lw=0,color='darkorchid',label='High Confidence')\n",
    "    \n",
    "    \n",
    "    #SIGNAL TO NOISE RATIO:\n",
    "    isort = np.argsort(SNR)\n",
    "    sc = ax[1].scatter(dt[isort],mds['heights']['h_ph'][isort],\n",
    "                        c=SNR[isort],s=size)\n",
    "\n",
    "    #-- add colorbar for scatter plot\n",
    "    cax = fig.add_axes([0.075, 0.080, 0.325, 0.02])\n",
    "    cbar = fig.colorbar(sc, cax=cax, extend='both', extendfrac=0.0375,\n",
    "    drawedges=False, orientation='horizontal')\n",
    "    cbar.solids.set_rasterized(True)     # -- rasterized colorbar to remove lines\n",
    "    cbar.ax.set_xlabel('Photon Classifier SNR')\n",
    "    cbar.ax.xaxis.set_label_position('top')\n",
    "    cbar.ax.tick_params(which='both',width=1,length=11,direction=\"in\")\n",
    "    \n",
    "    #LEGEND\n",
    "    lgd = ax[0].legend(loc=3,frameon=False)\n",
    "    lgd.get_frame().set_alpha(1.0)\n",
    "    for line in lgd.get_lines():\n",
    "        line.set_linewidth(6)\n",
    "    \n",
    "    #AXES:\n",
    "    ax[1].set_xlabel('Delta Time (s)',fontsize=10)\n",
    "    ax[0].set_ylabel('Photon Heights (m)',fontsize=10)\n",
    "    ax[1].set_ylabel('Photon Heights (m)',fontsize=10)\n",
    "    ax[0].set_title((pondID+': Photon Event Confidence'), fontsize=15)\n",
    "    ax[1].set_title((pondID+': Signal to Noise Ratio'), fontsize=15)\n",
    "                    \n",
    "    ax[1].set_xlim(dt_min, dt_max)\n",
    "    ax[0].set_ylim(h_avg - 10,h_avg+5)\n",
    "    ax[1].set_ylim(h_avg - 10,h_avg+5)\n",
    "    \n",
    "    #-- adjust the figure axes\n",
    "    fig.subplots_adjust(left=0.07, right=0.98, bottom=0.05, top=0.95, hspace=0.05)\n",
    "    \n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
